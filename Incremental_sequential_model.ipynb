{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convote Modeling\n",
    "- Data Main Page: http://www.cs.cornell.edu/home/llee/data/convote.html \n",
    "- About the Data: http://www.cs.cornell.edu/home/llee/data/convote/README.v1.1.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import os\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import regexp_tokenize, word_tokenize, sent_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Model Evaluation\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pickle files output from EDA phase\n",
    "train_data = pd.read_pickle('./train_data.pkl')\n",
    "test_data = pd.read_pickle('./test_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuition\n",
    "\n",
    "Political speech, regardless of political party, has ubiquitous syntactic and lexical characteristics. A speech classification model could be used to confirm this hypothesis. Can simple word counts and TF-IDF normalization language modeling techniques be used to delineate political parties?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "      <th>Party</th>\n",
       "      <th>Discussion</th>\n",
       "      <th>Vote</th>\n",
       "      <th>NumSents</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Total_tokens</th>\n",
       "      <th>Unique_tokens</th>\n",
       "      <th>lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>414_400080_3170075_DON</td>\n",
       "      <td>madam speaker , i yield myself 35 seconds . \\n...</td>\n",
       "      <td>DON</td>\n",
       "      <td>D</td>\n",
       "      <td>O</td>\n",
       "      <td>N</td>\n",
       "      <td>5</td>\n",
       "      <td>[35, seconds, want, give, sensenbrenner, benef...</td>\n",
       "      <td>53</td>\n",
       "      <td>46</td>\n",
       "      <td>[second, want, sensenbrenner, benefit, presump...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>414_400061_1909178_ROY</td>\n",
       "      <td>mr. chairman , i demand a recorded vote . \\n</td>\n",
       "      <td>ROY</td>\n",
       "      <td>R</td>\n",
       "      <td>O</td>\n",
       "      <td>Y</td>\n",
       "      <td>1</td>\n",
       "      <td>[demand, recorded, vote]</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[demand, record, vote]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>102_400175_0641038_ROY</td>\n",
       "      <td>mr. speaker , i rise today as a cosponsor of h...</td>\n",
       "      <td>ROY</td>\n",
       "      <td>R</td>\n",
       "      <td>O</td>\n",
       "      <td>Y</td>\n",
       "      <td>19</td>\n",
       "      <td>[rise, today, cosponsor, h.r, 8, support, rule...</td>\n",
       "      <td>161</td>\n",
       "      <td>111</td>\n",
       "      <td>[rise, today, cosponsor, support, rule, believ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>414_400080_3170065_DON</td>\n",
       "      <td>madam speaker , i yield myself 15 seconds . \\n...</td>\n",
       "      <td>DON</td>\n",
       "      <td>D</td>\n",
       "      <td>O</td>\n",
       "      <td>N</td>\n",
       "      <td>4</td>\n",
       "      <td>[15, seconds, let, remind, friend, returned, c...</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>[second, let, remind, friend, return, californ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>472_400314_2238033_DON</td>\n",
       "      <td>mr. speaker , i thank the ranking member , the...</td>\n",
       "      <td>DON</td>\n",
       "      <td>D</td>\n",
       "      <td>O</td>\n",
       "      <td>N</td>\n",
       "      <td>61</td>\n",
       "      <td>[thank, ranking, member, new, york, ms., slaug...</td>\n",
       "      <td>656</td>\n",
       "      <td>398</td>\n",
       "      <td>[thank, rank, member, new, york, ms, slaughter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1754</th>\n",
       "      <td>414_400134_1909144_ROY</td>\n",
       "      <td>mr. chairman , i demand a recorded vote . \\n</td>\n",
       "      <td>ROY</td>\n",
       "      <td>R</td>\n",
       "      <td>O</td>\n",
       "      <td>Y</td>\n",
       "      <td>1</td>\n",
       "      <td>[demand, recorded, vote]</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[demand, record, vote]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1755</th>\n",
       "      <td>204_400391_1201178_DON</td>\n",
       "      <td>mr. speaker , i yield 1 minute to the gentlema...</td>\n",
       "      <td>DON</td>\n",
       "      <td>D</td>\n",
       "      <td>O</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, minute, oregon, wu, xz4004370]</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>[minute, oregon, wu]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1756</th>\n",
       "      <td>506_400322_2390057_ROY</td>\n",
       "      <td>mr. chairman , i yield 5 minutes to the gentle...</td>\n",
       "      <td>ROY</td>\n",
       "      <td>R</td>\n",
       "      <td>O</td>\n",
       "      <td>Y</td>\n",
       "      <td>1</td>\n",
       "      <td>[5, california, cardoza, xz4000650]</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[california, cardoza]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1757</th>\n",
       "      <td>506_400035_2390095_RON</td>\n",
       "      <td>mr. chairman , i yield 1 1/2 minutes to the ge...</td>\n",
       "      <td>RON</td>\n",
       "      <td>R</td>\n",
       "      <td>O</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 1/2, connecticut, shays, xz4003700]</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>[connecticut, shays]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1758</th>\n",
       "      <td>102_400371_0642077_DON</td>\n",
       "      <td>mr. speaker , i thank the gentleman from north...</td>\n",
       "      <td>DON</td>\n",
       "      <td>D</td>\n",
       "      <td>O</td>\n",
       "      <td>N</td>\n",
       "      <td>39</td>\n",
       "      <td>[thank, north, dakota, yielding, perhaps, ment...</td>\n",
       "      <td>370</td>\n",
       "      <td>216</td>\n",
       "      <td>[thank, north, dakota, yield, mention, good, b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1759 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        File  \\\n",
       "0     414_400080_3170075_DON   \n",
       "1     414_400061_1909178_ROY   \n",
       "2     102_400175_0641038_ROY   \n",
       "3     414_400080_3170065_DON   \n",
       "4     472_400314_2238033_DON   \n",
       "...                      ...   \n",
       "1754  414_400134_1909144_ROY   \n",
       "1755  204_400391_1201178_DON   \n",
       "1756  506_400322_2390057_ROY   \n",
       "1757  506_400035_2390095_RON   \n",
       "1758  102_400371_0642077_DON   \n",
       "\n",
       "                                                   Text Label Party  \\\n",
       "0     madam speaker , i yield myself 35 seconds . \\n...   DON     D   \n",
       "1          mr. chairman , i demand a recorded vote . \\n   ROY     R   \n",
       "2     mr. speaker , i rise today as a cosponsor of h...   ROY     R   \n",
       "3     madam speaker , i yield myself 15 seconds . \\n...   DON     D   \n",
       "4     mr. speaker , i thank the ranking member , the...   DON     D   \n",
       "...                                                 ...   ...   ...   \n",
       "1754       mr. chairman , i demand a recorded vote . \\n   ROY     R   \n",
       "1755  mr. speaker , i yield 1 minute to the gentlema...   DON     D   \n",
       "1756  mr. chairman , i yield 5 minutes to the gentle...   ROY     R   \n",
       "1757  mr. chairman , i yield 1 1/2 minutes to the ge...   RON     R   \n",
       "1758  mr. speaker , i thank the gentleman from north...   DON     D   \n",
       "\n",
       "     Discussion Vote  NumSents  \\\n",
       "0             O    N         5   \n",
       "1             O    Y         1   \n",
       "2             O    Y        19   \n",
       "3             O    N         4   \n",
       "4             O    N        61   \n",
       "...         ...  ...       ...   \n",
       "1754          O    Y         1   \n",
       "1755          O    N         1   \n",
       "1756          O    Y         1   \n",
       "1757          O    N         1   \n",
       "1758          O    N        39   \n",
       "\n",
       "                                                 Tokens  Total_tokens  \\\n",
       "0     [35, seconds, want, give, sensenbrenner, benef...            53   \n",
       "1                              [demand, recorded, vote]             3   \n",
       "2     [rise, today, cosponsor, h.r, 8, support, rule...           161   \n",
       "3     [15, seconds, let, remind, friend, returned, c...            27   \n",
       "4     [thank, ranking, member, new, york, ms., slaug...           656   \n",
       "...                                                 ...           ...   \n",
       "1754                           [demand, recorded, vote]             3   \n",
       "1755                 [1, minute, oregon, wu, xz4004370]             5   \n",
       "1756                [5, california, cardoza, xz4000650]             4   \n",
       "1757            [1, 1/2, connecticut, shays, xz4003700]             5   \n",
       "1758  [thank, north, dakota, yielding, perhaps, ment...           370   \n",
       "\n",
       "      Unique_tokens                                             lemmas  \n",
       "0                46  [second, want, sensenbrenner, benefit, presump...  \n",
       "1                 3                             [demand, record, vote]  \n",
       "2               111  [rise, today, cosponsor, support, rule, believ...  \n",
       "3                27  [second, let, remind, friend, return, californ...  \n",
       "4               398  [thank, rank, member, new, york, ms, slaughter...  \n",
       "...             ...                                                ...  \n",
       "1754              3                             [demand, record, vote]  \n",
       "1755              5                               [minute, oregon, wu]  \n",
       "1756              4                              [california, cardoza]  \n",
       "1757              5                               [connecticut, shays]  \n",
       "1758            216  [thank, north, dakota, yield, mention, good, b...  \n",
       "\n",
       "[1759 rows x 11 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parititioning\n",
    "X_train, X_test, y_train, y_test = train_data['Text'], test_data['Text'], train_data['Party'], test_data['Party']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5660, 23120)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count vectorization\n",
    "count_vect = CountVectorizer(stop_words='english', strip_accents='unicode')\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5660, 23120)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF transformation\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data transformations\n",
    "X_test_counts = count_vect.transform(X_test)\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT = DecisionTreeClassifier(max_depth = 2).fit(X_train_tfidf, y_train) \n",
    "DT_predictions = DT.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.557134735645253"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy\n",
    "DT.score(X_test_tfidf, y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear kernel\n",
    "svm_linear = SVC(kernel = 'linear', C = 1).fit(X_train_tfidf, y_train) \n",
    "svm_predictions = svm_linear.predict(X_test_tfidf)\n",
    "\n",
    "# sigmoid kernel\n",
    "svm_sigmoid = SVC(kernel='sigmoid').fit(X_train_tfidf, y_train)\n",
    "svm_sigmoid_preds = svm_sigmoid.predict(X_test_tfidf)\n",
    "\n",
    "# polynomial kernel\n",
    "svm_poly = SVC(kernel='poly').fit(X_train_tfidf, y_train)\n",
    "svm_poly_preds = svm_poly.predict(X_test_tfidf)\n",
    "\n",
    "# radial bias function\n",
    "svm_rbf = SVC(kernel='rbf').fit(X_train_tfidf, y_train)\n",
    "svm_rbf_preds = svm_rbf.predict(X_test_tfidf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear: 0.7106310403638431\n",
      "Sigmoid: 0.6964184195565662\n",
      "Polynomial: 0.694144400227402\n",
      "RBF: 0.7265491756679932\n"
     ]
    }
   ],
   "source": [
    "print('Linear:', svm_linear.score(X_test_tfidf, y_test))\n",
    "print('Sigmoid:', svm_sigmoid.score(X_test_tfidf, y_test))\n",
    "print('Polynomial:', svm_poly.score(X_test_tfidf, y_test))\n",
    "print('RBF:', svm_rbf.score(X_test_tfidf, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Radial Bias Kernel is the most (accuracy) performant of the SVM classifiers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGDClassifier(loss = 'hinge', alpha=1e-3, random_state=42, max_iter=5, tol=None).fit(X_train_tfidf, y_train)\n",
    "sgd_predictions = sgd.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-nearest neighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = 7).fit(X_train_tfidf, y_train)\n",
    "knn_predictions = knn.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6009096077316657"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy\n",
    "knn.score(X_test_tfidf, y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "NBclf = MultinomialNB().fit(X_train_tfidf, y_train)\n",
    "NB_predictions = NBclf.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6668561682774303"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy\n",
    "NBclf.score(X_test_tfidf, y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[111   0 752]\n",
      " [  3   0   2]\n",
      " [ 22   0 869]] \n",
      " Decision Tree\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           D       0.82      0.13      0.22       863\n",
      "           I       0.00      0.00      0.00         5\n",
      "           R       0.54      0.98      0.69       891\n",
      "\n",
      "    accuracy                           0.56      1759\n",
      "   macro avg       0.45      0.37      0.30      1759\n",
      "weighted avg       0.67      0.56      0.46      1759\n",
      " \n",
      "\n",
      "\n",
      "[[633   0 230]\n",
      " [  3   0   2]\n",
      " [274   0 617]] \n",
      " SVM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           D       0.70      0.73      0.71       863\n",
      "           I       0.00      0.00      0.00         5\n",
      "           R       0.73      0.69      0.71       891\n",
      "\n",
      "    accuracy                           0.71      1759\n",
      "   macro avg       0.47      0.48      0.47      1759\n",
      "weighted avg       0.71      0.71      0.71      1759\n",
      " \n",
      "\n",
      "\n",
      "[[603   0 260]\n",
      " [  4   0   1]\n",
      " [254   0 637]] \n",
      " SGD\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           D       0.70      0.70      0.70       863\n",
      "           I       0.00      0.00      0.00         5\n",
      "           R       0.71      0.71      0.71       891\n",
      "\n",
      "    accuracy                           0.70      1759\n",
      "   macro avg       0.47      0.47      0.47      1759\n",
      "weighted avg       0.70      0.70      0.70      1759\n",
      " \n",
      "\n",
      "\n",
      "[[779   0  84]\n",
      " [  5   0   0]\n",
      " [613   0 278]] \n",
      " KNN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           D       0.56      0.90      0.69       863\n",
      "           I       0.00      0.00      0.00         5\n",
      "           R       0.77      0.31      0.44       891\n",
      "\n",
      "    accuracy                           0.60      1759\n",
      "   macro avg       0.44      0.40      0.38      1759\n",
      "weighted avg       0.66      0.60      0.56      1759\n",
      " \n",
      "\n",
      "\n",
      "[[694   0 169]\n",
      " [  4   0   1]\n",
      " [412   0 479]] \n",
      " MultiNB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           D       0.63      0.80      0.70       863\n",
      "           I       0.00      0.00      0.00         5\n",
      "           R       0.74      0.54      0.62       891\n",
      "\n",
      "    accuracy                           0.67      1759\n",
      "   macro avg       0.45      0.45      0.44      1759\n",
      "weighted avg       0.68      0.67      0.66      1759\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DT\n",
    "print(metrics.confusion_matrix(y_test, DT_predictions), '\\n', 'Decision Tree')\n",
    "print(metrics.classification_report(y_test, DT_predictions), '\\n\\n')\n",
    "\n",
    "# SVM\n",
    "print(metrics.confusion_matrix(y_test, svm_predictions), '\\n', 'SVM')\n",
    "print(metrics.classification_report(y_test, svm_predictions), '\\n\\n')\n",
    "\n",
    "# SGD\n",
    "print(metrics.confusion_matrix(y_test, sgd_predictions), '\\n', 'SGD')\n",
    "print(metrics.classification_report(y_test, sgd_predictions), '\\n\\n')\n",
    "\n",
    "# KNN\n",
    "print(metrics.confusion_matrix(y_test, knn_predictions), '\\n', 'KNN')\n",
    "print(metrics.classification_report(y_test, knn_predictions), '\\n\\n')\n",
    "\n",
    "# MultiNB\n",
    "print(metrics.confusion_matrix(y_test, NB_predictions), '\\n', 'MultiNB')\n",
    "print(metrics.classification_report(y_test, NB_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments\n",
    "\n",
    "The Support Vector Classifier with radial bias function performed the best, in terms of accuracy, among all the classifiers. The highest accuracy achieved was ~72% but there remains the issue of Independent class recognition. This meets our aforementioned hypothesis that political parties can not be separated by mere word-frequency information. Therefore, more advanced techniques will be required to understand word-relationships that are party-indepdendent but can still generate acceptable speeches that align with party tone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vijay S Chauhan\\Anaconda3\\lib\\site-packages\\past\\types\\oldstr.py:5: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Iterable\n",
      "C:\\Users\\Vijay S Chauhan\\Anaconda3\\lib\\site-packages\\past\\builtins\\misc.py:4: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Mapping\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "import numpy as np\n",
    "import spacy\n",
    "from gensim.models import CoherenceModel, LdaModel, LsiModel, HdpModel\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "from gensim.corpora import Dictionary\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim import models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatized token counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine lemmas, ignore empty lists due to short speeches of all stopwords\n",
    "def combine_lemmas(preprocess_lists):\n",
    "    total_lemma_tokens = []\n",
    "    \n",
    "    for lemma_list in preprocess_lists:\n",
    "        if not lemma_list:\n",
    "            pass\n",
    "        else:\n",
    "            total_lemma_tokens.append(lemma_list)\n",
    "    return total_lemma_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine_lemmas(train_data[train_data.Party == 'R']['lemmas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "      <th>Party</th>\n",
       "      <th>Discussion</th>\n",
       "      <th>Vote</th>\n",
       "      <th>NumSents</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Total_tokens</th>\n",
       "      <th>Unique_tokens</th>\n",
       "      <th>lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>282_400436_1413023_DMN</td>\n",
       "      <td>mr. speaker , i would like to say a word about...</td>\n",
       "      <td>DMN</td>\n",
       "      <td>D</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>17</td>\n",
       "      <td>[would, like, say, word, illinois, also, proba...</td>\n",
       "      <td>167</td>\n",
       "      <td>127</td>\n",
       "      <td>[word, illinois, probably, people, opposite, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>088_400272_2994052_DON</td>\n",
       "      <td>mr. speaker , today we have some very clear ch...</td>\n",
       "      <td>DON</td>\n",
       "      <td>D</td>\n",
       "      <td>O</td>\n",
       "      <td>N</td>\n",
       "      <td>16</td>\n",
       "      <td>[today, clear, choices, every, day, face, blac...</td>\n",
       "      <td>196</td>\n",
       "      <td>149</td>\n",
       "      <td>[today, clear, choice, day, face, black, white...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>038_400080_0251064_DON</td>\n",
       "      <td>mr. speaker , i yield myself such time as i ma...</td>\n",
       "      <td>DON</td>\n",
       "      <td>D</td>\n",
       "      <td>O</td>\n",
       "      <td>N</td>\n",
       "      <td>15</td>\n",
       "      <td>[may, consume, would, like, briefly, describe,...</td>\n",
       "      <td>152</td>\n",
       "      <td>111</td>\n",
       "      <td>[consume, briefly, describe, substitute, super...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>132_400227_0763073_DON</td>\n",
       "      <td>mr. chairman , i yield back the balance of my ...</td>\n",
       "      <td>DON</td>\n",
       "      <td>D</td>\n",
       "      <td>O</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>[back, balance]</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[balance]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>282_400380_1838049_ROY</td>\n",
       "      <td>madam chairman , will the gentleman yield ? \\n</td>\n",
       "      <td>ROY</td>\n",
       "      <td>R</td>\n",
       "      <td>O</td>\n",
       "      <td>Y</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     File                                               Text  \\\n",
       "0  282_400436_1413023_DMN  mr. speaker , i would like to say a word about...   \n",
       "1  088_400272_2994052_DON  mr. speaker , today we have some very clear ch...   \n",
       "2  038_400080_0251064_DON  mr. speaker , i yield myself such time as i ma...   \n",
       "3  132_400227_0763073_DON  mr. chairman , i yield back the balance of my ...   \n",
       "4  282_400380_1838049_ROY     madam chairman , will the gentleman yield ? \\n   \n",
       "\n",
       "  Label Party Discussion Vote  NumSents  \\\n",
       "0   DMN     D          M    N        17   \n",
       "1   DON     D          O    N        16   \n",
       "2   DON     D          O    N        15   \n",
       "3   DON     D          O    N         1   \n",
       "4   ROY     R          O    Y         1   \n",
       "\n",
       "                                              Tokens  Total_tokens  \\\n",
       "0  [would, like, say, word, illinois, also, proba...           167   \n",
       "1  [today, clear, choices, every, day, face, blac...           196   \n",
       "2  [may, consume, would, like, briefly, describe,...           152   \n",
       "3                                    [back, balance]             2   \n",
       "4                                                 []             0   \n",
       "\n",
       "   Unique_tokens                                             lemmas  \n",
       "0            127  [word, illinois, probably, people, opposite, w...  \n",
       "1            149  [today, clear, choice, day, face, black, white...  \n",
       "2            111  [consume, briefly, describe, substitute, super...  \n",
       "3              2                                          [balance]  \n",
       "4              0                                                 []  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of lemma tokens by party \n",
    "ind_lemmas = combine_lemmas(train_data[train_data.Party == 'I']['lemmas'])\n",
    "dem_lemmas = combine_lemmas(train_data[train_data.Party == 'D']['lemmas'])\n",
    "rep_lemmas = combine_lemmas(train_data[train_data.Party == 'R']['lemmas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of lemma tokens by party Tokens\n",
    "ind_tokens = combine_lemmas(train_data[train_data.Party == 'I']['Tokens'])\n",
    "dem_tokens = combine_lemmas(train_data[train_data.Party == 'D']['Tokens'])\n",
    "rep_tokens = combine_lemmas(train_data[train_data.Party == 'R']['Tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phraser Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Gensim method to auto-detect multi-word phrases/ngram collocations \n",
    "from a stream of sentences; output ngram:count frequency mapping\n",
    "'''\n",
    "ind_bigram = gensim.models.Phrases(ind_lemmas, min_count=2)\n",
    "dem_bigram = gensim.models.Phrases(dem_lemmas, min_count=2)\n",
    "rep_bigram = gensim.models.Phrases(rep_lemmas, min_count=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ind_bigram.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Semantic Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim is creating an LSI topic model for each Party and output top 5 topic vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_LSI(model, bigram, lemma_tokens):\n",
    "    texts = [bigram[line] for line in lemma_tokens]\n",
    "    dictionary = Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    \n",
    "    model_output = model(corpus=corpus, id2word=dictionary, num_topics=10)\n",
    "    return model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Independent topics: \n",
      "\t [(0, '0.401*\"job\" + 0.261*\"go\" + 0.213*\"middle_class\" + 0.207*\"china\" + 0.195*\"trade\"'), (1, '-0.230*\"think\" + 0.207*\"today\" + 0.190*\"job\" + 0.189*\"year\" + -0.189*\"go\"'), (2, '0.274*\"job\" + -0.220*\"pension\" + -0.200*\"say\" + -0.185*\"international\" + -0.164*\"rule\"'), (3, '-0.494*\"pension\" + -0.314*\"cash_balance\" + -0.270*\"conversion\" + -0.225*\"old\" + -0.135*\"provision\"'), (4, '0.229*\"good\" + 0.203*\"agreement\" + 0.170*\"lose\" + 0.165*\"america\" + 0.165*\"low_wage\"')] \n",
      "\n",
      "\n",
      "Democrat topics: \n",
      "\t [(0, '0.499*\"bill\" + 0.228*\"state\" + 0.171*\"law\" + 0.153*\"amendment\" + 0.144*\"year\"'), (1, '0.402*\"law\" + 0.393*\"state\" + 0.195*\"case\" + 0.181*\"attorney_general\" + -0.177*\"amendment\"'), (2, '-0.331*\"amendment\" + 0.224*\"program\" + 0.212*\"bill\" + -0.196*\"house\" + 0.194*\"gt\"'), (3, '-0.495*\"bill\" + 0.333*\"nbsp_amp\" + 0.318*\"gt\" + 0.304*\"lt_br\" + 0.207*\"state\"'), (4, '-0.320*\"professor_law\" + 0.291*\"state\" + 0.236*\"bill\" + -0.222*\"bankruptcy\" + -0.217*\"law\"')] \n",
      "\n",
      "\n",
      "Republican topics: \n",
      "\t [(0, '0.410*\"bill\" + 0.188*\"committee\" + 0.182*\"year\" + 0.166*\"state\" + 0.161*\"program\"'), (1, '0.434*\"committee\" + 0.264*\"shall\" + 0.220*\"security\" + 0.201*\"member\" + 0.197*\"jurisdiction\"'), (2, '0.406*\"state\" + -0.321*\"program\" + 0.255*\"law\" + -0.209*\"budget\" + -0.170*\"committee\"'), (3, '-0.667*\"bill\" + 0.162*\"member\" + 0.148*\"shall\" + 0.138*\"budget\" + 0.127*\"come\"'), (4, '-0.381*\"budget\" + -0.287*\"fiscal_year\" + 0.260*\"bill\" + -0.239*\"nbsp_amp\" + -0.218*\"year\"')] \n",
      "\n",
      "\n",
      "Wall time: 8.71 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Independent\n",
    "lsi_ind = generate_LSI(LsiModel,ind_bigram,ind_lemmas)\n",
    "print('Independent topics:', '\\n\\t',lsi_ind.show_topics(num_topics=5, num_words=5), '\\n\\n')\n",
    "\n",
    "# Democrat\n",
    "lsi_dem = generate_LSI(LsiModel,dem_bigram,dem_lemmas)\n",
    "print('Democrat topics:', '\\n\\t',lsi_dem.show_topics(num_topics=5, num_words=5), '\\n\\n')\n",
    "\n",
    "#Republican\n",
    "lsi_rep = generate_LSI(LsiModel,rep_bigram,rep_lemmas)\n",
    "print('Republican topics:', '\\n\\t',lsi_rep.show_topics(num_topics=5, num_words=5), '\\n\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_LDA_model(model, bigram, lemma_tokens):\n",
    "    texts = [bigram[line] for line in lemma_tokens]\n",
    "    \n",
    "    dictionary = Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    model_output = model(corpus=corpus, id2word=dictionary, \n",
    "                         num_topics=10, passes=10, iterations=100, chunksize=50e3, update_every=2)\n",
    "    return model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Independent topics: \n",
      "\t [(5, '0.002*\"pension\" + 0.002*\"conversion\" + 0.002*\"old\" + 0.002*\"cash_balance\" + 0.002*\"worker\"'), (8, '0.025*\"job\" + 0.024*\"go\" + 0.020*\"middle_class\" + 0.018*\"think\" + 0.016*\"work\"'), (6, '0.015*\"say\" + 0.015*\"job\" + 0.013*\"china\" + 0.013*\"go\" + 0.013*\"united_states\"'), (2, '0.017*\"pleased\" + 0.017*\"utah\" + 0.017*\"bishop\" + 0.002*\"china\" + 0.002*\"go\"'), (9, '0.017*\"amplify\" + 0.017*\"wish\" + 0.017*\"ohio_kucinich\" + 0.002*\"job\" + 0.002*\"middle_class\"')] \n",
      "\n",
      "\n",
      "Democrat topics: \n",
      "\t [(8, '0.025*\"bill\" + 0.022*\"energy\" + 0.012*\"oil\" + 0.009*\"amendment\" + 0.007*\"price\"'), (4, '0.013*\"bill\" + 0.008*\"program\" + 0.008*\"cut\" + 0.007*\"amendment\" + 0.006*\"year\"'), (2, '0.011*\"amendment\" + 0.011*\"bill\" + 0.009*\"member\" + 0.009*\"house\" + 0.008*\"vote\"'), (6, '0.025*\"bill\" + 0.014*\"state\" + 0.007*\"law\" + 0.006*\"people\" + 0.006*\"legislation\"'), (7, '0.021*\"program\" + 0.014*\"budget\" + 0.012*\"fund\" + 0.009*\"need\" + 0.008*\"housing\"')] \n",
      "\n",
      "\n",
      "Republican topics: \n",
      "\t [(0, '0.010*\"year\" + 0.008*\"people\" + 0.008*\"go\" + 0.008*\"budget\" + 0.008*\"want\"'), (2, '0.013*\"amendment\" + 0.009*\"committee\" + 0.008*\"state\" + 0.006*\"think\" + 0.006*\"member\"'), (1, '0.009*\"bill\" + 0.007*\"amendment\" + 0.006*\"provide\" + 0.005*\"nbsp_amp\" + 0.005*\"need\"'), (6, '0.013*\"amendment\" + 0.012*\"program\" + 0.009*\"bill\" + 0.009*\"think\" + 0.006*\"vote\"'), (4, '0.011*\"state\" + 0.010*\"bill\" + 0.008*\"act\" + 0.008*\"support\" + 0.006*\"law\"')] \n",
      "\n",
      "\n",
      "Wall time: 2min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Independent\n",
    "lda_ind = generate_LDA_model(LdaModel,ind_bigram,ind_lemmas)\n",
    "print('Independent topics:', '\\n\\t',lda_ind.show_topics(num_topics=5, num_words=5), '\\n\\n')\n",
    "\n",
    "# Democrat\n",
    "lda_dem = generate_LDA_model(LdaModel,dem_bigram,dem_lemmas)\n",
    "print('Democrat topics:', '\\n\\t',lda_dem.show_topics(num_topics=5, num_words=5), '\\n\\n')\n",
    "\n",
    "\n",
    "# Republican\n",
    "lda_rep = generate_LDA_model(LdaModel,rep_bigram,rep_lemmas)\n",
    "print('Republican topics:', '\\n\\t',lda_rep.show_topics(num_topics=5, num_words=5), '\\n\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heirarchical Dirichlet Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_HDP(model, bigram, lemma_tokens):\n",
    "    texts = [bigram[line] for line in lemma_tokens]\n",
    "    \n",
    "    dictionary = Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    model_output = model(corpus=corpus, id2word=dictionary, max_time=120)\n",
    "    return model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Independent topics: \n",
      "\t [(0, '0.031*job + 0.026*go + 0.022*middle_class + 0.020*think + 0.018*work'), (1, '0.036*job + 0.020*today + 0.018*china + 0.018*year + 0.018*trade'), (2, '0.017*say + 0.017*international + 0.017*go + 0.014*labor + 0.014*wto'), (3, '0.043*job + 0.020*lose + 0.020*good + 0.016*let + 0.016*million'), (4, '0.076*pension + 0.048*cash_balance + 0.041*conversion + 0.034*old + 0.027*worker')] \n",
      "\n",
      "\n",
      "Democrat topics: \n",
      "\t [(0, '0.016*bill + 0.008*state + 0.005*amendment + 0.005*people + 0.005*need'), (1, '0.014*bill + 0.006*amendment + 0.006*year + 0.006*need + 0.005*people'), (2, '0.009*bill + 0.007*need + 0.006*committee + 0.006*support + 0.006*program'), (3, '0.008*bill + 0.006*amendment + 0.005*vote + 0.005*program + 0.005*year'), (4, '0.012*bill + 0.006*amendment + 0.006*federal + 0.005*state + 0.005*death_penalty')] \n",
      "\n",
      "\n",
      "Republican topics: \n",
      "\t [(0, '0.012*bill + 0.006*amendment + 0.006*support + 0.006*year + 0.005*need'), (1, '0.015*bill + 0.007*state + 0.006*committee + 0.006*year + 0.005*work'), (2, '0.010*bill + 0.007*amendment + 0.006*small_business + 0.005*committee + 0.005*year'), (3, '0.017*nbsp_amp + 0.012*amp_nbsp + 0.009*lt_p + 0.009*fiscal_year + 0.008*gt_amp'), (4, '0.006*bill + 0.006*law + 0.004*committee + 0.004*member + 0.004*reform')] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "hdp_ind = generate_HDP(HdpModel,ind_bigram,ind_lemmas)\n",
    "print('Independent topics:', '\\n\\t',hdp_ind.show_topics(num_topics=5, num_words=5), '\\n\\n')\n",
    "\n",
    "hdp_dem = generate_HDP(HdpModel,dem_bigram,dem_lemmas)\n",
    "print('Democrat topics:', '\\n\\t',hdp_dem.show_topics(num_topics=5, num_words=5), '\\n\\n')\n",
    "\n",
    "hdp_rep = generate_HDP(HdpModel,rep_bigram,rep_lemmas)\n",
    "print('Republican topics:', '\\n\\t',hdp_rep.show_topics(num_topics=5, num_words=5), '\\n\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_bar_graph(coherences, indices):\n",
    "    \"\"\"\n",
    "    Function to plot bar graph.\n",
    "    coherences: list of coherence values\n",
    "    indices: Indices to be used to mark bars. Length of this and coherences should be equal.\n",
    "    \"\"\"\n",
    "    assert len(coherences) == len(indices)\n",
    "    n = len(coherences)\n",
    "    x = np.arange(n)\n",
    "    plt.bar(x, coherences, width=0.2, tick_label=indices, align='center')\n",
    "    plt.xlabel('Models')\n",
    "    plt.ylabel('Coherence Value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluation(lsimodel,ldamodel,hdpmodel, bigrams, lemma_tokens):\n",
    "    texts = [bigrams[line] for line in lemma_tokens]\n",
    "    dictionary = Dictionary(texts)\n",
    "    \n",
    "    lsitopics = [[word for word, prob in topic] for topicid, topic in lsimodel.show_topics(num_topics=5, num_words=5, formatted=False)]\n",
    "    hdptopics = [[word for word, prob in topic] for topicid, topic in hdpmodel.show_topics(num_topics=5, num_words=5, formatted=False)]\n",
    "    ldatopics = [[word for word, prob in topic] for topicid, topic in ldamodel.show_topics(num_topics=5, num_words=5, formatted=False)]\n",
    "    \n",
    "    lsi_coherence = CoherenceModel(topics=lsitopics[:10], texts=texts, dictionary=dictionary, window_size=10).get_coherence()\n",
    "    lda_coherence = CoherenceModel(topics=ldatopics[:10], texts=texts, dictionary=dictionary, window_size=10).get_coherence()\n",
    "    hdp_coherence = CoherenceModel(topics=hdptopics[:10], texts=texts, dictionary=dictionary, window_size=10).get_coherence()\n",
    "    \n",
    "    \n",
    "    evaluate_bar_graph([lsi_coherence, lda_coherence, hdp_coherence],\n",
    "                     ['LSI', 'LDA', 'HDP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Independent\n",
    "# model_evaluation(lsi_ind, lda_ind, hdp_ind, ind_bigram, ind_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Democrat\n",
    "model_evaluation(lsi_dem, lda_dem, hdp_dem, dem_bigram, dem_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Republican\n",
    "# model_evaluation(lsi_rep, lda_rep, hdp_rep, rep_bigram, rep_lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.manifold import TSNE\n",
    "import re\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_vect = Word2Vec(ind_tokens, window=3, size=300, workers=4, min_count=3,iter=500)\n",
    "dem_vect = Word2Vec(dem_tokens, window=3, size=300, workers=4, min_count=3, iter=500)\n",
    "rep_vect = Word2Vec(rep_tokens, window=3, size=300, workers=4, min_count=3, iter=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('big', 0.6902073621749878), ('important', 0.6569638252258301), ('forced', 0.6504098176956177), ('said', 0.5756253004074097), ('look', 0.5576599836349487)] \n",
      "\n",
      " [('house', 0.5109204053878784), ('bill', 0.4339698553085327), ('committee', 0.39469119906425476), ('majority', 0.3682865500450134), ('think', 0.36376094818115234)] \n",
      "\n",
      " [('house', 0.4060817062854767), ('bill', 0.3543917238712311), ('committee', 0.33553454279899597), ('legislation', 0.3193507790565491), ('would', 0.30283012986183167)]\n"
     ]
    }
   ],
   "source": [
    "# How good is each party-vector embedding?\n",
    "print(ind_vect.most_similar('congress', topn=5), '\\n\\n', \n",
    "      dem_vect.most_similar('congress', topn=5), '\\n\\n',\n",
    "      rep_vect.most_similar('congress', topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating model for Independent candidate speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(input_dim=ind_vect.wv.vectors.shape[0], \n",
    "                            output_dim=ind_vect.wv.vectors.shape[1], \n",
    "                            weights=[ind_vect.wv.vectors])\n",
    "\n",
    "model_ind = Sequential()\n",
    "\n",
    "model_ind.add(embedding_layer)\n",
    "model_ind.add(LSTM(ind_vect.wv.vectors.shape[1]))\n",
    "model_ind.add(Dense(ind_vect.wv.vectors.shape[0]))   \n",
    "model_ind.add(Activation('softmax'))\n",
    "model_ind.compile(optimizer='adam', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating model for Democratic candidate speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(input_dim=dem_vect.wv.vectors.shape[0], \n",
    "                            output_dim=dem_vect.wv.vectors.shape[1], \n",
    "                            weights=[dem_vect.wv.vectors])\n",
    "\n",
    "model_dem = Sequential()\n",
    "\n",
    "model_dem.add(embedding_layer)\n",
    "model_dem.add(LSTM(ind_vect.wv.vectors.shape[1]))\n",
    "model_dem.add(Dense(ind_vect.wv.vectors.shape[0]))   \n",
    "model_dem.add(Activation('softmax'))\n",
    "model_dem.compile(optimizer='adam', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating model for Republic candidate speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(input_dim=rep_vect.wv.vectors.shape[0], \n",
    "                            output_dim=rep_vect.wv.vectors.shape[1], \n",
    "                            weights=[rep_vect.wv.vectors])\n",
    "\n",
    "model_rep = Sequential()\n",
    "\n",
    "model_rep.add(embedding_layer)\n",
    "model_rep.add(LSTM(ind_vect.wv.vectors.shape[1]))\n",
    "model_rep.add(Dense(ind_vect.wv.vectors.shape[0]))   \n",
    "model_rep.add(Activation('softmax'))c\n",
    "model_rep.compile(optimizer='adam', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, None, 300)         66300     \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 300)               721200    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 221)               66521     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 221)               0         \n",
      "=================================================================\n",
      "Total params: 854,021\n",
      "Trainable params: 854,021\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_ind.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, None, 300)         3101100   \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 300)               721200    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 221)               66521     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 221)               0         \n",
      "=================================================================\n",
      "Total params: 3,888,821\n",
      "Trainable params: 3,888,821\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_dem.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, None, 300)         2564400   \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 300)               721200    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 221)               66521     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 221)               0         \n",
      "=================================================================\n",
      "Total params: 3,352,121\n",
      "Trainable params: 3,352,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_rep.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2idx(word,word_model):\n",
    "    return word_model.wv.vocab[word].index\n",
    "\n",
    "def idx2word(idx,word_model):\n",
    "    return word_model.wv.index2word[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result embedding shape: (221, 300)\n",
      "Checking similar words:\n",
      "  congress -> big (0.69), important (0.66), forced (0.64), said (0.57), look (0.56), vermont (0.51), legislation (0.50), agreements (0.44)\n"
     ]
    }
   ],
   "source": [
    "pretrained_weights = ind_vect.wv.vectors\n",
    "vocab_size, emdedding_size = pretrained_weights.shape\n",
    "print('Result embedding shape:', pretrained_weights.shape)\n",
    "print('Checking similar words:')\n",
    "for word in ['congress']:\n",
    "  most_similar = ', '.join('%s (%.2f)' % (similar, dist)\n",
    "                           for similar, dist in ind_vect.wv.most_similar(word)[:8])\n",
    "  print('  %s -> %s' % (word, most_similar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    if temperature <= 0:\n",
    "        return np.argmax(preds)\n",
    "    \n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    \n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def generate_next(text, num_generated,word_vec,model):\n",
    "    word_idxs = [word2idx(word,word_vec) for word in text.lower().split()]\n",
    "    for i in range(num_generated):\n",
    "        prediction = model.predict(x=np.array(word_idxs))\n",
    "        idx = sample(prediction[-1], temperature=0.7)\n",
    "        word_idxs.append(idx)\n",
    "    return ' '.join(idx2word(idx,word_vec) for idx in word_idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate text for independent candidate speech on job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'job rich really legislation talked yes us ohio good-paying make got benefits aarp gap working debate'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_next('job', 15,ind_vect,model_ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate text for democratic candidate speech on job cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'job cut great substitute members commission workers safety americans process troops amendment 2 ask united see congressional'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_next('job cut', 15,dem_vect,model_dem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate text for republic candidate speech on job cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'job cut within 1 congress since percent could families 2 leadership jurisdiction families military rights us billion'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_next('job cut', 15,rep_vect,model_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "### We are using two techniques for evaluation our model :-\n",
    "### 1. BLEU Score\n",
    "### 2. ROUGE metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence BLEU Score\n",
    "\n",
    "To analyze the model performance,I have taken a sentence from the test dataset, which talks about job cut and I want to see what kind of text is produced by the model. It calculates individual n-gram scores at all orders from 1 to n and weighting them by calculating the weighted geometric mean.\n",
    "\n",
    "By default, the sentence_bleu() and corpus_bleu() scores calculate the cumulative 4-gram BLEU score, also called BLEU-4.\n",
    "The weights for the BLEU-4 are 1/4 (25%) or 0.25 for each of the 1-gram, 2-gram, 3-gram and 4-gram scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2250738585072626e-308"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference =['job cut is something nobody would like to talk about and certainly nobody would like to encourage'.split()]\n",
    "Candidate = generate_next('job cut', 15,rep_vect,model_rep)\n",
    "candidate = Candidate.split()\n",
    "score = sentence_bleu(reference, candidate, weights=(1, 0, 0, 0))\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROUGE (metric)\n",
    "ROUGE, or Recall-Oriented Understudy for Gisting Evaluation is a set of metrics used for evaluating natural language processing output generated by a model. The metrics compare and produce summary of translated text against a reference or a set of references (human-produced) summary or translation.\n",
    "\n",
    "ROUGE-N: Overlap of N-grams between the system and reference summaries.</br>\n",
    "ROUGE-1 refers to the overlap of unigram (each word) between the system and reference summaries.</br>\n",
    "ROUGE-2 refers to the overlap of bigrams between the system and reference summaries.</br>\n",
    "ROUGE-l refers to  Longest Common Subsequence (LCS) based statistics. Longest common subsequence problem takes into account sentence level structure similarity naturally and identifies longest co-occurring in sequence n-grams automatically.</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'rouge-1': {'f': 0.05882352441176514, 'p': 0.058823529411764705, 'r': 0.058823529411764705}, 'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0}, 'rouge-l': {'f': 0.0689655122948874, 'p': 0.0625, 'r': 0.07692307692307693}}]\n"
     ]
    }
   ],
   "source": [
    "hypothesis = generate_next('job cut', 15,rep_vect,model_rep)\n",
    "\n",
    "reference = \"job cuts is something nobody would like to talk about and certainly nobody would like to encourage\"\n",
    "\n",
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(hypothesis, reference)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "193px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
